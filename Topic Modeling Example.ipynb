{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "In this workbook we loosely follow the example from \"Toward Data Science\" on\n",
    "[Topic Modeling with spaCy and gensim](https://towardsdatascience.com/building-a-topic-modeling-pipeline-with-spacy-and-gensim-c5dc03ffc619). First, we need to install gensim, so open up a command window (and I had to do it in \"administrator\"\n",
    "mode) and run this command: `pip install gensim`. We're also going to do some data viz, so run `pip install pyLDAvis`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions we'll use later in the topic modeling.\n",
    "\n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them. \n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "    \n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to Know the Brown Corpus\n",
    "\n",
    "Let's spend a bit of time getting to know what's in the Brown corpus, our NLTK example of an \"overlapping\" corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories of articles in Brown corpus\n",
    "print(brown.categories())\n",
    "\n",
    "for category in brown.categories() :\n",
    "    print(f\"For {category} we have {len(brown.fileids(categories=category))} articles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a list of the articles in of editorial, government, news, and romance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_modeling = []\n",
    "\n",
    "for category in ['editorial','government','news','romance'] :\n",
    "    for file_id in brown.fileids(categories=category) :\n",
    "        text = brown.words(fileids=file_id)\n",
    "        for_modeling.append(\" \".join(text))\n",
    "        \n",
    "print(f\"We have {len(for_modeling)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates spaCy's default stop words list with my additional words. \n",
    "stop_list = ['`',\"Mr.\",\"Mrs.\",\"Ms.\"]\n",
    "nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "# Iterates over the words in the stop words list and resets the \"is_stop\" flag.\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp.add_pipe(lemmatizer,name='lemmatizer',after='ner')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "\n",
    "# Iterates through each article in the corpus.\n",
    "for doc in for_modeling :\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(doc)\n",
    "    doc_list.append([t.lower() for t in pr if t.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of word IDs to words.\n",
    "words = corpora.Dictionary(doc_list)\n",
    "\n",
    "# Turns each document into a bag of words.\n",
    "corpus = [words.doc2bow(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual fitting of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=15,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics(num_words=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda_model, corpus, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our topic classifications by document and see how good a job LDA is doing recovering our original topics. We'll take each document one at a time, parse it (as a joined string), and do basically the same processing as we did before. \n",
    "\n",
    "You can pass the processed document into the LDA model using square brackets (this is a bit odd) and recieve a tuple back. The first element of the tuple contains the topics and associated probabilities. The max probability will be the assigned topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_assignments = []\n",
    "\n",
    "for file_id in brown.fileids(categories=\"romance\") :\n",
    "    doc = brown.words(fileids=file_id)\n",
    "    pr = nlp(\" \".join(doc))\n",
    "    doc = [t.lower() for t in pr if t.isalpha()]\n",
    "    doc_new = words.doc2bow(doc)\n",
    "    \n",
    "    topic_probs = lda_model[doc_new][0]\n",
    "    topic = max(topic_probs,key=lambda x: x[1])\n",
    "    topic_assignments.append(topic[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at those topic assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(topic_assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like topic zero is overwhelmingly romance. Let's do this for every category we worked with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_assignments = defaultdict(list)\n",
    "\n",
    "for category in ['editorial','government','news','romance'] :\n",
    "    for file_id in brown.fileids(categories=category) :\n",
    "\n",
    "        doc = brown.words(fileids=file_id)\n",
    "        pr = nlp(\" \".join(doc))\n",
    "        doc = [t.lower() for t in pr if t.isalpha()]\n",
    "        doc_new = words.doc2bow(doc)\n",
    "\n",
    "        topic_probs = lda_model[doc_new][0]\n",
    "        topic = max(topic_probs,key=lambda x: x[1])\n",
    "        topic_assignments[category].append(topic[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat, topic_list in topic_assignments.items() :\n",
    "    print(f\"In {cat} we had the following:\")\n",
    "    topic_count = Counter(topic_list).most_common()\n",
    "    \n",
    "    for topic, count in topic_count : \n",
    "        print(f\"    {count} articles were classified as topic {topic}.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this assignment is pretty imperfect, though the categories overlap pretty heavily, particularly the first three. Romance seems to be safely identified on its own. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
